{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data for linear regression\n",
    "# set random seed\n",
    "np.random.seed(9)\n",
    "# draw 100 random numbers from uniform dist [0, 1]\n",
    "x = np.random.uniform(0, 1, (100, 1))\n",
    "# draw random noise from standard normal\n",
    "z = np.random.normal(0, .1, (100, 1))\n",
    "# create ground truth for y = 8x - 3\n",
    "y = 3 * x - 1 + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test\n",
    "x_train, y_train = x[:80], y[:80]\n",
    "x_val, y_val = x[80:], y[80:]\n",
    "\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# move data from numpy to torch\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "x_val_tensor = torch.from_numpy(x_val).float().to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val).float().to(device)\n",
    "print(type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0688], requires_grad=True) tensor([-0.6226], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create trainable parameters for the model\n",
    "weight = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "bias = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "print(weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ epoch ] 0\n",
      "[ training ] training loss = 4.024172782897949\n",
      "[ eval ] validation loss = 1.9201282262802124\n",
      "[ epoch ] 1\n",
      "[ training ] training loss = 2.627333164215088\n",
      "[ eval ] validation loss = 1.339868187904358\n",
      "[ epoch ] 2\n",
      "[ training ] training loss = 1.8359190225601196\n",
      "[ eval ] validation loss = 1.0363456010818481\n",
      "[ epoch ] 3\n",
      "[ training ] training loss = 1.3837190866470337\n",
      "[ eval ] validation loss = 0.8800220489501953\n",
      "[ epoch ] 4\n",
      "[ training ] training loss = 1.1216756105422974\n",
      "[ eval ] validation loss = 0.8004370927810669\n",
      "[ epoch ] 5\n",
      "[ training ] training loss = 0.9663246273994446\n",
      "[ eval ] validation loss = 0.7596678137779236\n",
      "[ epoch ] 6\n",
      "[ training ] training loss = 0.870928168296814\n",
      "[ eval ] validation loss = 0.7376121282577515\n",
      "[ epoch ] 7\n",
      "[ training ] training loss = 0.8093125224113464\n",
      "[ eval ] validation loss = 0.7238497734069824\n",
      "[ epoch ] 8\n",
      "[ training ] training loss = 0.7668172121047974\n",
      "[ eval ] validation loss = 0.7131634950637817\n",
      "[ epoch ] 9\n",
      "[ training ] training loss = 0.7352281808853149\n",
      "[ eval ] validation loss = 0.7030874490737915\n",
      "[ epoch ] 10\n",
      "[ training ] training loss = 0.7099422812461853\n",
      "[ eval ] validation loss = 0.6925770044326782\n",
      "[ epoch ] 11\n",
      "[ training ] training loss = 0.6883763670921326\n",
      "[ eval ] validation loss = 0.6812947988510132\n",
      "[ epoch ] 12\n",
      "[ training ] training loss = 0.6690798997879028\n",
      "[ eval ] validation loss = 0.6692336797714233\n",
      "[ epoch ] 13\n",
      "[ training ] training loss = 0.6512355208396912\n",
      "[ eval ] validation loss = 0.6565235257148743\n",
      "[ epoch ] 14\n",
      "[ training ] training loss = 0.6343814134597778\n",
      "[ eval ] validation loss = 0.6433347463607788\n",
      "[ epoch ] 15\n",
      "[ training ] training loss = 0.6182544827461243\n",
      "[ eval ] validation loss = 0.6298342347145081\n",
      "[ epoch ] 16\n",
      "[ training ] training loss = 0.6027027368545532\n",
      "[ eval ] validation loss = 0.6161670088768005\n",
      "[ epoch ] 17\n",
      "[ training ] training loss = 0.5876370668411255\n",
      "[ eval ] validation loss = 0.6024518013000488\n",
      "[ epoch ] 18\n",
      "[ training ] training loss = 0.573003888130188\n",
      "[ eval ] validation loss = 0.588781476020813\n",
      "[ epoch ] 19\n",
      "[ training ] training loss = 0.5587685108184814\n",
      "[ eval ] validation loss = 0.5752270817756653\n",
      "[ epoch ] 20\n",
      "[ training ] training loss = 0.5449080467224121\n",
      "[ eval ] validation loss = 0.5618410706520081\n",
      "[ epoch ] 21\n",
      "[ training ] training loss = 0.5314053893089294\n",
      "[ eval ] validation loss = 0.5486616492271423\n",
      "[ epoch ] 22\n",
      "[ training ] training loss = 0.5182477235794067\n",
      "[ eval ] validation loss = 0.5357158184051514\n",
      "[ epoch ] 23\n",
      "[ training ] training loss = 0.505423903465271\n",
      "[ eval ] validation loss = 0.5230216383934021\n",
      "[ epoch ] 24\n",
      "[ training ] training loss = 0.4929242730140686\n",
      "[ eval ] validation loss = 0.5105911493301392\n",
      "[ epoch ] 25\n",
      "[ training ] training loss = 0.48074015974998474\n",
      "[ eval ] validation loss = 0.4984310567378998\n",
      "[ epoch ] 26\n",
      "[ training ] training loss = 0.46886301040649414\n",
      "[ eval ] validation loss = 0.4865446090698242\n",
      "[ epoch ] 27\n",
      "[ training ] training loss = 0.45728468894958496\n",
      "[ eval ] validation loss = 0.4749327600002289\n",
      "[ epoch ] 28\n",
      "[ training ] training loss = 0.445997953414917\n",
      "[ eval ] validation loss = 0.4635940492153168\n",
      "[ epoch ] 29\n",
      "[ training ] training loss = 0.4349953234195709\n",
      "[ eval ] validation loss = 0.45252570509910583\n",
      "[ epoch ] 30\n",
      "[ training ] training loss = 0.42426949739456177\n",
      "[ eval ] validation loss = 0.44172415137290955\n",
      "[ epoch ] 31\n",
      "[ training ] training loss = 0.41381344199180603\n",
      "[ eval ] validation loss = 0.4311848282814026\n",
      "[ epoch ] 32\n",
      "[ training ] training loss = 0.40362048149108887\n",
      "[ eval ] validation loss = 0.4209030568599701\n",
      "[ epoch ] 33\n",
      "[ training ] training loss = 0.39368385076522827\n",
      "[ eval ] validation loss = 0.4108734726905823\n",
      "[ epoch ] 34\n",
      "[ training ] training loss = 0.38399720191955566\n",
      "[ eval ] validation loss = 0.40109091997146606\n",
      "[ epoch ] 35\n",
      "[ training ] training loss = 0.374554306268692\n",
      "[ eval ] validation loss = 0.39154961705207825\n",
      "[ epoch ] 36\n",
      "[ training ] training loss = 0.36534884572029114\n",
      "[ eval ] validation loss = 0.3822442293167114\n",
      "[ epoch ] 37\n",
      "[ training ] training loss = 0.35637497901916504\n",
      "[ eval ] validation loss = 0.3731691241264343\n",
      "[ epoch ] 38\n",
      "[ training ] training loss = 0.3476269245147705\n",
      "[ eval ] validation loss = 0.3643190264701843\n",
      "[ epoch ] 39\n",
      "[ training ] training loss = 0.33909890055656433\n",
      "[ eval ] validation loss = 0.35568827390670776\n",
      "[ epoch ] 40\n",
      "[ training ] training loss = 0.3307853937149048\n",
      "[ eval ] validation loss = 0.34727171063423157\n",
      "[ epoch ] 41\n",
      "[ training ] training loss = 0.3226810693740845\n",
      "[ eval ] validation loss = 0.33906397223472595\n",
      "[ epoch ] 42\n",
      "[ training ] training loss = 0.3147805333137512\n",
      "[ eval ] validation loss = 0.3310599625110626\n",
      "[ epoch ] 43\n",
      "[ training ] training loss = 0.30707868933677673\n",
      "[ eval ] validation loss = 0.32325464487075806\n",
      "[ epoch ] 44\n",
      "[ training ] training loss = 0.2995707094669342\n",
      "[ eval ] validation loss = 0.31564319133758545\n",
      "[ epoch ] 45\n",
      "[ training ] training loss = 0.29225149750709534\n",
      "[ eval ] validation loss = 0.30822059512138367\n",
      "[ epoch ] 46\n",
      "[ training ] training loss = 0.2851164937019348\n",
      "[ eval ] validation loss = 0.30098241567611694\n",
      "[ epoch ] 47\n",
      "[ training ] training loss = 0.2781608998775482\n",
      "[ eval ] validation loss = 0.2939237952232361\n",
      "[ epoch ] 48\n",
      "[ training ] training loss = 0.27138036489486694\n",
      "[ eval ] validation loss = 0.28704047203063965\n",
      "[ epoch ] 49\n",
      "[ training ] training loss = 0.2647703289985657\n",
      "[ eval ] validation loss = 0.2803279459476471\n",
      "[ epoch ] 50\n",
      "[ training ] training loss = 0.25832659006118774\n",
      "[ eval ] validation loss = 0.27378201484680176\n",
      "[ epoch ] 51\n",
      "[ training ] training loss = 0.2520449459552765\n",
      "[ eval ] validation loss = 0.26739856600761414\n",
      "[ epoch ] 52\n",
      "[ training ] training loss = 0.245921328663826\n",
      "[ eval ] validation loss = 0.26117342710494995\n",
      "[ epoch ] 53\n",
      "[ training ] training loss = 0.23995175957679749\n",
      "[ eval ] validation loss = 0.2551026940345764\n",
      "[ epoch ] 54\n",
      "[ training ] training loss = 0.2341322898864746\n",
      "[ eval ] validation loss = 0.2491825520992279\n",
      "[ epoch ] 55\n",
      "[ training ] training loss = 0.22845931351184845\n",
      "[ eval ] validation loss = 0.24340927600860596\n",
      "[ epoch ] 56\n",
      "[ training ] training loss = 0.2229290008544922\n",
      "[ eval ] validation loss = 0.23777905106544495\n",
      "[ epoch ] 57\n",
      "[ training ] training loss = 0.21753773093223572\n",
      "[ eval ] validation loss = 0.2322884351015091\n",
      "[ epoch ] 58\n",
      "[ training ] training loss = 0.21228215098381042\n",
      "[ eval ] validation loss = 0.22693386673927307\n",
      "[ epoch ] 59\n",
      "[ training ] training loss = 0.20715877413749695\n",
      "[ eval ] validation loss = 0.22171199321746826\n",
      "[ epoch ] 60\n",
      "[ training ] training loss = 0.20216426253318787\n",
      "[ eval ] validation loss = 0.21661953628063202\n",
      "[ epoch ] 61\n",
      "[ training ] training loss = 0.1972954273223877\n",
      "[ eval ] validation loss = 0.2116531878709793\n",
      "[ epoch ] 62\n",
      "[ training ] training loss = 0.19254903495311737\n",
      "[ eval ] validation loss = 0.2068098783493042\n",
      "[ epoch ] 63\n",
      "[ training ] training loss = 0.18792201578617096\n",
      "[ eval ] validation loss = 0.20208647847175598\n",
      "[ epoch ] 64\n",
      "[ training ] training loss = 0.18341144919395447\n",
      "[ eval ] validation loss = 0.1974799931049347\n",
      "[ epoch ] 65\n",
      "[ training ] training loss = 0.17901431024074554\n",
      "[ eval ] validation loss = 0.19298754632472992\n",
      "[ epoch ] 66\n",
      "[ training ] training loss = 0.17472782731056213\n",
      "[ eval ] validation loss = 0.18860629200935364\n",
      "[ epoch ] 67\n",
      "[ training ] training loss = 0.17054912447929382\n",
      "[ eval ] validation loss = 0.18433339893817902\n",
      "[ epoch ] 68\n",
      "[ training ] training loss = 0.1664755642414093\n",
      "[ eval ] validation loss = 0.18016621470451355\n",
      "[ epoch ] 69\n",
      "[ training ] training loss = 0.16250446438789368\n",
      "[ eval ] validation loss = 0.17610211670398712\n",
      "[ epoch ] 70\n",
      "[ training ] training loss = 0.1586332619190216\n",
      "[ eval ] validation loss = 0.17213848233222961\n",
      "[ epoch ] 71\n",
      "[ training ] training loss = 0.15485945343971252\n",
      "[ eval ] validation loss = 0.16827282309532166\n",
      "[ epoch ] 72\n",
      "[ training ] training loss = 0.15118058025836945\n",
      "[ eval ] validation loss = 0.16450272500514984\n",
      "[ epoch ] 73\n",
      "[ training ] training loss = 0.14759419858455658\n",
      "[ eval ] validation loss = 0.16082577407360077\n",
      "[ epoch ] 74\n",
      "[ training ] training loss = 0.14409807324409485\n",
      "[ eval ] validation loss = 0.1572396457195282\n",
      "[ epoch ] 75\n",
      "[ training ] training loss = 0.140689879655838\n",
      "[ eval ] validation loss = 0.15374211966991425\n",
      "[ epoch ] 76\n",
      "[ training ] training loss = 0.13736747205257416\n",
      "[ eval ] validation loss = 0.15033096075057983\n",
      "[ epoch ] 77\n",
      "[ training ] training loss = 0.13412858545780182\n",
      "[ eval ] validation loss = 0.14700400829315186\n",
      "[ epoch ] 78\n",
      "[ training ] training loss = 0.13097116351127625\n",
      "[ eval ] validation loss = 0.14375916123390198\n",
      "[ epoch ] 79\n",
      "[ training ] training loss = 0.12789323925971985\n",
      "[ eval ] validation loss = 0.14059437811374664\n",
      "[ epoch ] 80\n",
      "[ training ] training loss = 0.12489267438650131\n",
      "[ eval ] validation loss = 0.13750767707824707\n",
      "[ epoch ] 81\n",
      "[ training ] training loss = 0.121967613697052\n",
      "[ eval ] validation loss = 0.13449710607528687\n",
      "[ epoch ] 82\n",
      "[ training ] training loss = 0.11911611258983612\n",
      "[ eval ] validation loss = 0.1315608024597168\n",
      "[ epoch ] 83\n",
      "[ training ] training loss = 0.116336390376091\n",
      "[ eval ] validation loss = 0.12869684398174286\n",
      "[ epoch ] 84\n",
      "[ training ] training loss = 0.11362655460834503\n",
      "[ eval ] validation loss = 0.12590348720550537\n",
      "[ epoch ] 85\n",
      "[ training ] training loss = 0.11098487675189972\n",
      "[ eval ] validation loss = 0.12317894399166107\n",
      "[ epoch ] 86\n",
      "[ training ] training loss = 0.10840968787670135\n",
      "[ eval ] validation loss = 0.12052153050899506\n",
      "[ epoch ] 87\n",
      "[ training ] training loss = 0.10589925944805145\n",
      "[ eval ] validation loss = 0.11792954057455063\n",
      "[ epoch ] 88\n",
      "[ training ] training loss = 0.10345198959112167\n",
      "[ eval ] validation loss = 0.11540138721466064\n",
      "[ epoch ] 89\n",
      "[ training ] training loss = 0.10106627643108368\n",
      "[ eval ] validation loss = 0.11293546855449677\n",
      "[ epoch ] 90\n",
      "[ training ] training loss = 0.0987405776977539\n",
      "[ eval ] validation loss = 0.11053021252155304\n",
      "[ epoch ] 91\n",
      "[ training ] training loss = 0.09647337347269058\n",
      "[ eval ] validation loss = 0.10818411409854889\n",
      "[ epoch ] 92\n",
      "[ training ] training loss = 0.0942632183432579\n",
      "[ eval ] validation loss = 0.10589573532342911\n",
      "[ epoch ] 93\n",
      "[ training ] training loss = 0.09210864454507828\n",
      "[ eval ] validation loss = 0.1036636084318161\n",
      "[ epoch ] 94\n",
      "[ training ] training loss = 0.09000828117132187\n",
      "[ eval ] validation loss = 0.10148636996746063\n",
      "[ epoch ] 95\n",
      "[ training ] training loss = 0.08796074241399765\n",
      "[ eval ] validation loss = 0.09936260432004929\n",
      "[ epoch ] 96\n",
      "[ training ] training loss = 0.08596472442150116\n",
      "[ eval ] validation loss = 0.0972910150885582\n",
      "[ epoch ] 97\n",
      "[ training ] training loss = 0.08401890844106674\n",
      "[ eval ] validation loss = 0.0952703133225441\n",
      "[ epoch ] 98\n",
      "[ training ] training loss = 0.08212205022573471\n",
      "[ eval ] validation loss = 0.09329919517040253\n",
      "[ epoch ] 99\n",
      "[ training ] training loss = 0.0802728608250618\n",
      "[ eval ] validation loss = 0.09137649089097977\n",
      "[ epoch ] 100\n",
      "[ training ] training loss = 0.07847024500370026\n",
      "[ eval ] validation loss = 0.08950092643499374\n",
      "[ epoch ] 101\n",
      "[ training ] training loss = 0.0767129510641098\n",
      "[ eval ] validation loss = 0.08767138421535492\n",
      "[ epoch ] 102\n",
      "[ training ] training loss = 0.0749998465180397\n",
      "[ eval ] validation loss = 0.08588670194149017\n",
      "[ epoch ] 103\n",
      "[ training ] training loss = 0.0733298659324646\n",
      "[ eval ] validation loss = 0.08414576947689056\n",
      "[ epoch ] 104\n",
      "[ training ] training loss = 0.07170189917087555\n",
      "[ eval ] validation loss = 0.08244749903678894\n",
      "[ epoch ] 105\n",
      "[ training ] training loss = 0.070114865899086\n",
      "[ eval ] validation loss = 0.08079082518815994\n",
      "[ epoch ] 106\n",
      "[ training ] training loss = 0.06856773048639297\n",
      "[ eval ] validation loss = 0.07917472720146179\n",
      "[ epoch ] 107\n",
      "[ training ] training loss = 0.06705953925848007\n",
      "[ eval ] validation loss = 0.0775981992483139\n",
      "[ epoch ] 108\n",
      "[ training ] training loss = 0.06558931618928909\n",
      "[ eval ] validation loss = 0.07606025040149689\n",
      "[ epoch ] 109\n",
      "[ training ] training loss = 0.06415604054927826\n",
      "[ eval ] validation loss = 0.07455992698669434\n",
      "[ epoch ] 110\n",
      "[ training ] training loss = 0.06275883316993713\n",
      "[ eval ] validation loss = 0.07309632003307343\n",
      "[ epoch ] 111\n",
      "[ training ] training loss = 0.0613967701792717\n",
      "[ eval ] validation loss = 0.07166846841573715\n",
      "[ epoch ] 112\n",
      "[ training ] training loss = 0.06006897613406181\n",
      "[ eval ] validation loss = 0.07027550786733627\n",
      "[ epoch ] 113\n",
      "[ training ] training loss = 0.058774590492248535\n",
      "[ eval ] validation loss = 0.06891659647226334\n",
      "[ epoch ] 114\n",
      "[ training ] training loss = 0.05751274153590202\n",
      "[ eval ] validation loss = 0.0675908774137497\n",
      "[ epoch ] 115\n",
      "[ training ] training loss = 0.0562826469540596\n",
      "[ eval ] validation loss = 0.0662975162267685\n",
      "[ epoch ] 116\n",
      "[ training ] training loss = 0.055083490908145905\n",
      "[ eval ] validation loss = 0.06503572314977646\n",
      "[ epoch ] 117\n",
      "[ training ] training loss = 0.05391450598835945\n",
      "[ eval ] validation loss = 0.06380470097064972\n",
      "[ epoch ] 118\n",
      "[ training ] training loss = 0.05277493596076965\n",
      "[ eval ] validation loss = 0.06260371208190918\n",
      "[ epoch ] 119\n",
      "[ training ] training loss = 0.05166402459144592\n",
      "[ eval ] validation loss = 0.061431996524333954\n",
      "[ epoch ] 120\n",
      "[ training ] training loss = 0.05058106780052185\n",
      "[ eval ] validation loss = 0.06028882414102554\n",
      "[ epoch ] 121\n",
      "[ training ] training loss = 0.04952532798051834\n",
      "[ eval ] validation loss = 0.05917349457740784\n",
      "[ epoch ] 122\n",
      "[ training ] training loss = 0.04849616438150406\n",
      "[ eval ] validation loss = 0.05808534100651741\n",
      "[ epoch ] 123\n",
      "[ training ] training loss = 0.04749288409948349\n",
      "[ eval ] validation loss = 0.05702364444732666\n",
      "[ epoch ] 124\n",
      "[ training ] training loss = 0.046514835208654404\n",
      "[ eval ] validation loss = 0.055987805128097534\n",
      "[ epoch ] 125\n",
      "[ training ] training loss = 0.04556141048669815\n",
      "[ eval ] validation loss = 0.054977141320705414\n",
      "[ epoch ] 126\n",
      "[ training ] training loss = 0.044631946831941605\n",
      "[ eval ] validation loss = 0.053991056978702545\n",
      "[ epoch ] 127\n",
      "[ training ] training loss = 0.04372585937380791\n",
      "[ eval ] validation loss = 0.053028929978609085\n",
      "[ epoch ] 128\n",
      "[ training ] training loss = 0.042842574417591095\n",
      "[ eval ] validation loss = 0.052090175449848175\n",
      "[ epoch ] 129\n",
      "[ training ] training loss = 0.04198151454329491\n",
      "[ eval ] validation loss = 0.05117420479655266\n",
      "[ epoch ] 130\n",
      "[ training ] training loss = 0.04114212095737457\n",
      "[ eval ] validation loss = 0.05028047040104866\n",
      "[ epoch ] 131\n",
      "[ training ] training loss = 0.04032382741570473\n",
      "[ eval ] validation loss = 0.04940841346979141\n",
      "[ epoch ] 132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ training ] training loss = 0.03952612355351448\n",
      "[ eval ] validation loss = 0.048557497560977936\n",
      "[ epoch ] 133\n",
      "[ training ] training loss = 0.03874849155545235\n",
      "[ eval ] validation loss = 0.047727204859256744\n",
      "[ epoch ] 134\n",
      "[ training ] training loss = 0.03799041360616684\n",
      "[ eval ] validation loss = 0.046917036175727844\n",
      "[ epoch ] 135\n",
      "[ training ] training loss = 0.03725142031908035\n",
      "[ eval ] validation loss = 0.04612647742033005\n",
      "[ epoch ] 136\n",
      "[ training ] training loss = 0.036531005054712296\n",
      "[ eval ] validation loss = 0.04535505920648575\n",
      "[ epoch ] 137\n",
      "[ training ] training loss = 0.03582870960235596\n",
      "[ eval ] validation loss = 0.04460228979587555\n",
      "[ epoch ] 138\n",
      "[ training ] training loss = 0.03514408692717552\n",
      "[ eval ] validation loss = 0.04386772960424423\n",
      "[ epoch ] 139\n",
      "[ training ] training loss = 0.034476667642593384\n",
      "[ eval ] validation loss = 0.043150924146175385\n",
      "[ epoch ] 140\n",
      "[ training ] training loss = 0.033826060593128204\n",
      "[ eval ] validation loss = 0.04245143011212349\n",
      "[ epoch ] 141\n",
      "[ training ] training loss = 0.033191803842782974\n",
      "[ eval ] validation loss = 0.041768841445446014\n",
      "[ epoch ] 142\n",
      "[ training ] training loss = 0.03257351741194725\n",
      "[ eval ] validation loss = 0.041102707386016846\n",
      "[ epoch ] 143\n",
      "[ training ] training loss = 0.03197077289223671\n",
      "[ eval ] validation loss = 0.04045265167951584\n",
      "[ epoch ] 144\n",
      "[ training ] training loss = 0.031383197754621506\n",
      "[ eval ] validation loss = 0.03981827199459076\n",
      "[ epoch ] 145\n",
      "[ training ] training loss = 0.030810397118330002\n",
      "[ eval ] validation loss = 0.03919918090105057\n",
      "[ epoch ] 146\n",
      "[ training ] training loss = 0.03025200590491295\n",
      "[ eval ] validation loss = 0.038594990968704224\n",
      "[ epoch ] 147\n",
      "[ training ] training loss = 0.0297076553106308\n",
      "[ eval ] validation loss = 0.038005344569683075\n",
      "[ epoch ] 148\n",
      "[ training ] training loss = 0.029176991432905197\n",
      "[ eval ] validation loss = 0.03742988780140877\n",
      "[ epoch ] 149\n",
      "[ training ] training loss = 0.028659721836447716\n",
      "[ eval ] validation loss = 0.03686826303601265\n",
      "[ epoch ] 150\n",
      "[ training ] training loss = 0.028155427426099777\n",
      "[ eval ] validation loss = 0.03632014989852905\n",
      "[ epoch ] 151\n",
      "[ training ] training loss = 0.027663830667734146\n",
      "[ eval ] validation loss = 0.03578520938754082\n",
      "[ epoch ] 152\n",
      "[ training ] training loss = 0.02718460001051426\n",
      "[ eval ] validation loss = 0.035263098776340485\n",
      "[ epoch ] 153\n",
      "[ training ] training loss = 0.026717424392700195\n",
      "[ eval ] validation loss = 0.03475351631641388\n",
      "[ epoch ] 154\n",
      "[ training ] training loss = 0.02626199647784233\n",
      "[ eval ] validation loss = 0.03425615653395653\n",
      "[ epoch ] 155\n",
      "[ training ] training loss = 0.025818025693297386\n",
      "[ eval ] validation loss = 0.03377072140574455\n",
      "[ epoch ] 156\n",
      "[ training ] training loss = 0.02538522146642208\n",
      "[ eval ] validation loss = 0.03329690918326378\n",
      "[ epoch ] 157\n",
      "[ training ] training loss = 0.02496330998837948\n",
      "[ eval ] validation loss = 0.03283444046974182\n",
      "[ epoch ] 158\n",
      "[ training ] training loss = 0.024552013725042343\n",
      "[ eval ] validation loss = 0.032383039593696594\n",
      "[ epoch ] 159\n",
      "[ training ] training loss = 0.024151060730218887\n",
      "[ eval ] validation loss = 0.03194243833422661\n",
      "[ epoch ] 160\n",
      "[ training ] training loss = 0.02376018464565277\n",
      "[ eval ] validation loss = 0.03151235356926918\n",
      "[ epoch ] 161\n",
      "[ training ] training loss = 0.02337915077805519\n",
      "[ eval ] validation loss = 0.03109254501760006\n",
      "[ epoch ] 162\n",
      "[ training ] training loss = 0.023007694631814957\n",
      "[ eval ] validation loss = 0.030682751908898354\n",
      "[ epoch ] 163\n",
      "[ training ] training loss = 0.02264558896422386\n",
      "[ eval ] validation loss = 0.03028274141252041\n",
      "[ epoch ] 164\n",
      "[ training ] training loss = 0.02229258418083191\n",
      "[ eval ] validation loss = 0.02989226020872593\n",
      "[ epoch ] 165\n",
      "[ training ] training loss = 0.02194846235215664\n",
      "[ eval ] validation loss = 0.02951108291745186\n",
      "[ epoch ] 166\n",
      "[ training ] training loss = 0.02161300554871559\n",
      "[ eval ] validation loss = 0.02913898229598999\n",
      "[ epoch ] 167\n",
      "[ training ] training loss = 0.02128598280251026\n",
      "[ eval ] validation loss = 0.02877572737634182\n",
      "[ epoch ] 168\n",
      "[ training ] training loss = 0.020967181771993637\n",
      "[ eval ] validation loss = 0.02842111326754093\n",
      "[ epoch ] 169\n",
      "[ training ] training loss = 0.0206564050167799\n",
      "[ eval ] validation loss = 0.028074929490685463\n",
      "[ epoch ] 170\n",
      "[ training ] training loss = 0.02035345509648323\n",
      "[ eval ] validation loss = 0.027736971154808998\n",
      "[ epoch ] 171\n",
      "[ training ] training loss = 0.020058106631040573\n",
      "[ eval ] validation loss = 0.02740701660513878\n",
      "[ epoch ] 172\n",
      "[ training ] training loss = 0.019770193845033646\n",
      "[ eval ] validation loss = 0.02708490751683712\n",
      "[ epoch ] 173\n",
      "[ training ] training loss = 0.019489532336592674\n",
      "[ eval ] validation loss = 0.026770418509840965\n",
      "[ epoch ] 174\n",
      "[ training ] training loss = 0.01921592466533184\n",
      "[ eval ] validation loss = 0.026463370770215988\n",
      "[ epoch ] 175\n",
      "[ training ] training loss = 0.018949193879961967\n",
      "[ eval ] validation loss = 0.026163598522543907\n",
      "[ epoch ] 176\n",
      "[ training ] training loss = 0.018689187243580818\n",
      "[ eval ] validation loss = 0.0258709155023098\n",
      "[ epoch ] 177\n",
      "[ training ] training loss = 0.018435711041092873\n",
      "[ eval ] validation loss = 0.025585150346159935\n",
      "[ epoch ] 178\n",
      "[ training ] training loss = 0.01818861812353134\n",
      "[ eval ] validation loss = 0.025306129828095436\n",
      "[ epoch ] 179\n",
      "[ training ] training loss = 0.0179477259516716\n",
      "[ eval ] validation loss = 0.02503368817269802\n",
      "[ epoch ] 180\n",
      "[ training ] training loss = 0.0177129115909338\n",
      "[ eval ] validation loss = 0.02476768009364605\n",
      "[ epoch ] 181\n",
      "[ training ] training loss = 0.017483994364738464\n",
      "[ eval ] validation loss = 0.024507936090230942\n",
      "[ epoch ] 182\n",
      "[ training ] training loss = 0.0172608382999897\n",
      "[ eval ] validation loss = 0.02425430528819561\n",
      "[ epoch ] 183\n",
      "[ training ] training loss = 0.01704329065978527\n",
      "[ eval ] validation loss = 0.024006638675928116\n",
      "[ epoch ] 184\n",
      "[ training ] training loss = 0.01683122292160988\n",
      "[ eval ] validation loss = 0.02376479282975197\n",
      "[ epoch ] 185\n",
      "[ training ] training loss = 0.016624482348561287\n",
      "[ eval ] validation loss = 0.023528626188635826\n",
      "[ epoch ] 186\n",
      "[ training ] training loss = 0.016422951593995094\n",
      "[ eval ] validation loss = 0.023298010230064392\n",
      "[ epoch ] 187\n",
      "[ training ] training loss = 0.01622648909687996\n",
      "[ eval ] validation loss = 0.023072797805070877\n",
      "[ epoch ] 188\n",
      "[ training ] training loss = 0.016034968197345734\n",
      "[ eval ] validation loss = 0.022852865979075432\n",
      "[ epoch ] 189\n",
      "[ training ] training loss = 0.01584826037287712\n",
      "[ eval ] validation loss = 0.022638076916337013\n",
      "[ epoch ] 190\n",
      "[ training ] training loss = 0.015666257590055466\n",
      "[ eval ] validation loss = 0.02242831513285637\n",
      "[ epoch ] 191\n",
      "[ training ] training loss = 0.015488827601075172\n",
      "[ eval ] validation loss = 0.022223467007279396\n",
      "[ epoch ] 192\n",
      "[ training ] training loss = 0.015315860509872437\n",
      "[ eval ] validation loss = 0.022023402154445648\n",
      "[ epoch ] 193\n",
      "[ training ] training loss = 0.015147234313189983\n",
      "[ eval ] validation loss = 0.02182799205183983\n",
      "[ epoch ] 194\n",
      "[ training ] training loss = 0.014982867054641247\n",
      "[ eval ] validation loss = 0.021637145429849625\n",
      "[ epoch ] 195\n",
      "[ training ] training loss = 0.014822621829807758\n",
      "[ eval ] validation loss = 0.021450741216540337\n",
      "[ epoch ] 196\n",
      "[ training ] training loss = 0.014666423201560974\n",
      "[ eval ] validation loss = 0.021268684417009354\n",
      "[ epoch ] 197\n",
      "[ training ] training loss = 0.014514136128127575\n",
      "[ eval ] validation loss = 0.021090853959321976\n",
      "[ epoch ] 198\n",
      "[ training ] training loss = 0.014365686103701591\n",
      "[ eval ] validation loss = 0.020917154848575592\n",
      "[ epoch ] 199\n",
      "[ training ] training loss = 0.01422097347676754\n",
      "[ eval ] validation loss = 0.020747492089867592\n",
      "[ epoch ] 200\n",
      "[ training ] training loss = 0.014079898595809937\n",
      "[ eval ] validation loss = 0.02058175951242447\n",
      "[ epoch ] 201\n",
      "[ training ] training loss = 0.013942371122539043\n",
      "[ eval ] validation loss = 0.02041986584663391\n",
      "[ epoch ] 202\n",
      "[ training ] training loss = 0.013808307237923145\n",
      "[ eval ] validation loss = 0.020261723548173904\n",
      "[ epoch ] 203\n",
      "[ training ] training loss = 0.013677616603672504\n",
      "[ eval ] validation loss = 0.020107241347432137\n",
      "[ epoch ] 204\n",
      "[ training ] training loss = 0.013550207018852234\n",
      "[ eval ] validation loss = 0.019956324249505997\n",
      "[ epoch ] 205\n",
      "[ training ] training loss = 0.01342600304633379\n",
      "[ eval ] validation loss = 0.019808895885944366\n",
      "[ epoch ] 206\n",
      "[ training ] training loss = 0.013304933905601501\n",
      "[ eval ] validation loss = 0.019664868712425232\n",
      "[ epoch ] 207\n",
      "[ training ] training loss = 0.013186909258365631\n",
      "[ eval ] validation loss = 0.01952414959669113\n",
      "[ epoch ] 208\n",
      "[ training ] training loss = 0.013071844354271889\n",
      "[ eval ] validation loss = 0.01938667520880699\n",
      "[ epoch ] 209\n",
      "[ training ] training loss = 0.0129596758633852\n",
      "[ eval ] validation loss = 0.019252361729741096\n",
      "[ epoch ] 210\n",
      "[ training ] training loss = 0.01285032369196415\n",
      "[ eval ] validation loss = 0.019121140241622925\n",
      "[ epoch ] 211\n",
      "[ training ] training loss = 0.012743731960654259\n",
      "[ eval ] validation loss = 0.018992923200130463\n",
      "[ epoch ] 212\n",
      "[ training ] training loss = 0.01263981033116579\n",
      "[ eval ] validation loss = 0.018867645412683487\n",
      "[ epoch ] 213\n",
      "[ training ] training loss = 0.012538516893982887\n",
      "[ eval ] validation loss = 0.018745237961411476\n",
      "[ epoch ] 214\n",
      "[ training ] training loss = 0.012439759448170662\n",
      "[ eval ] validation loss = 0.01862562820315361\n",
      "[ epoch ] 215\n",
      "[ training ] training loss = 0.012343489564955235\n",
      "[ eval ] validation loss = 0.018508756533265114\n",
      "[ epoch ] 216\n",
      "[ training ] training loss = 0.012249644845724106\n",
      "[ eval ] validation loss = 0.018394554033875465\n",
      "[ epoch ] 217\n",
      "[ training ] training loss = 0.012158161029219627\n",
      "[ eval ] validation loss = 0.018282946199178696\n",
      "[ epoch ] 218\n",
      "[ training ] training loss = 0.012068978510797024\n",
      "[ eval ] validation loss = 0.018173884600400925\n",
      "[ epoch ] 219\n",
      "[ training ] training loss = 0.011982043273746967\n",
      "[ eval ] validation loss = 0.018067311495542526\n",
      "[ epoch ] 220\n",
      "[ training ] training loss = 0.011897293850779533\n",
      "[ eval ] validation loss = 0.01796315610408783\n",
      "[ epoch ] 221\n",
      "[ training ] training loss = 0.011814671568572521\n",
      "[ eval ] validation loss = 0.017861371859908104\n",
      "[ epoch ] 222\n",
      "[ training ] training loss = 0.0117341298609972\n",
      "[ eval ] validation loss = 0.017761889845132828\n",
      "[ epoch ] 223\n",
      "[ training ] training loss = 0.01165560819208622\n",
      "[ eval ] validation loss = 0.017664672806859016\n",
      "[ epoch ] 224\n",
      "[ training ] training loss = 0.011579063721001148\n",
      "[ eval ] validation loss = 0.017569642513990402\n",
      "[ epoch ] 225\n",
      "[ training ] training loss = 0.011504456400871277\n",
      "[ eval ] validation loss = 0.017476771026849747\n",
      "[ epoch ] 226\n",
      "[ training ] training loss = 0.011431711725890636\n",
      "[ eval ] validation loss = 0.01738598756492138\n",
      "[ epoch ] 227\n",
      "[ training ] training loss = 0.011360805481672287\n",
      "[ eval ] validation loss = 0.017297256737947464\n",
      "[ epoch ] 228\n",
      "[ training ] training loss = 0.011291677132248878\n",
      "[ eval ] validation loss = 0.01721053197979927\n",
      "[ epoch ] 229\n",
      "[ training ] training loss = 0.01122429221868515\n",
      "[ eval ] validation loss = 0.017125749960541725\n",
      "[ epoch ] 230\n",
      "[ training ] training loss = 0.01115860603749752\n",
      "[ eval ] validation loss = 0.01704288087785244\n",
      "[ epoch ] 231\n",
      "[ training ] training loss = 0.011094561778008938\n",
      "[ eval ] validation loss = 0.01696186512708664\n",
      "[ epoch ] 232\n",
      "[ training ] training loss = 0.011032136157155037\n",
      "[ eval ] validation loss = 0.01688266359269619\n",
      "[ epoch ] 233\n",
      "[ training ] training loss = 0.01097127515822649\n",
      "[ eval ] validation loss = 0.016805235296487808\n",
      "[ epoch ] 234\n",
      "[ training ] training loss = 0.010911949910223484\n",
      "[ eval ] validation loss = 0.01672953926026821\n",
      "[ epoch ] 235\n",
      "[ training ] training loss = 0.010854115709662437\n",
      "[ eval ] validation loss = 0.016655540093779564\n",
      "[ epoch ] 236\n",
      "[ training ] training loss = 0.010797737166285515\n",
      "[ eval ] validation loss = 0.016583196818828583\n",
      "[ epoch ] 237\n",
      "[ training ] training loss = 0.01074278075248003\n",
      "[ eval ] validation loss = 0.01651245914399624\n",
      "[ epoch ] 238\n",
      "[ training ] training loss = 0.01068919524550438\n",
      "[ eval ] validation loss = 0.016443289816379547\n",
      "[ epoch ] 239\n",
      "[ training ] training loss = 0.010636969469487667\n",
      "[ eval ] validation loss = 0.016375664621591568\n",
      "[ epoch ] 240\n",
      "[ training ] training loss = 0.01058605220168829\n",
      "[ eval ] validation loss = 0.01630953699350357\n",
      "[ epoch ] 241\n",
      "[ training ] training loss = 0.010536417365074158\n",
      "[ eval ] validation loss = 0.01624487154185772\n",
      "[ epoch ] 242\n",
      "[ training ] training loss = 0.010488029569387436\n",
      "[ eval ] validation loss = 0.016181640326976776\n",
      "[ epoch ] 243\n",
      "[ training ] training loss = 0.010440857149660587\n",
      "[ eval ] validation loss = 0.016119815409183502\n",
      "[ epoch ] 244\n",
      "[ training ] training loss = 0.010394876822829247\n",
      "[ eval ] validation loss = 0.01605934649705887\n",
      "[ epoch ] 245\n",
      "[ training ] training loss = 0.010350042954087257\n",
      "[ eval ] validation loss = 0.016000214964151382\n",
      "[ epoch ] 246\n",
      "[ training ] training loss = 0.010306352749466896\n",
      "[ eval ] validation loss = 0.015942387282848358\n",
      "[ epoch ] 247\n",
      "[ training ] training loss = 0.010263746604323387\n",
      "[ eval ] validation loss = 0.01588582620024681\n",
      "[ epoch ] 248\n",
      "[ training ] training loss = 0.010222217999398708\n",
      "[ eval ] validation loss = 0.01583050936460495\n",
      "[ epoch ] 249\n",
      "[ training ] training loss = 0.010181737132370472\n",
      "[ eval ] validation loss = 0.01577640324831009\n",
      "[ epoch ] 250\n",
      "[ training ] training loss = 0.01014227606356144\n",
      "[ eval ] validation loss = 0.015723487362265587\n",
      "[ epoch ] 251\n",
      "[ training ] training loss = 0.010103801265358925\n",
      "[ eval ] validation loss = 0.015671730041503906\n",
      "[ epoch ] 252\n",
      "[ training ] training loss = 0.01006629690527916\n",
      "[ eval ] validation loss = 0.015621094033122063\n",
      "[ epoch ] 253\n",
      "[ training ] training loss = 0.01002973597496748\n",
      "[ eval ] validation loss = 0.01557157188653946\n",
      "[ epoch ] 254\n",
      "[ training ] training loss = 0.00999409519135952\n",
      "[ eval ] validation loss = 0.01552312821149826\n",
      "[ epoch ] 255\n",
      "[ training ] training loss = 0.00995935034006834\n",
      "[ eval ] validation loss = 0.015475733205676079\n",
      "[ epoch ] 256\n",
      "[ training ] training loss = 0.009925477206707\n",
      "[ eval ] validation loss = 0.015429362654685974\n",
      "[ epoch ] 257\n",
      "[ training ] training loss = 0.009892458096146584\n",
      "[ eval ] validation loss = 0.015384008176624775\n",
      "[ epoch ] 258\n",
      "[ training ] training loss = 0.009860268794000149\n",
      "[ eval ] validation loss = 0.015339625999331474\n",
      "[ epoch ] 259\n",
      "[ training ] training loss = 0.00982888787984848\n",
      "[ eval ] validation loss = 0.015296204015612602\n",
      "[ epoch ] 260\n",
      "[ training ] training loss = 0.009798300452530384\n",
      "[ eval ] validation loss = 0.015253718011081219\n",
      "[ epoch ] 261\n",
      "[ training ] training loss = 0.00976848229765892\n",
      "[ eval ] validation loss = 0.01521216332912445\n",
      "[ epoch ] 262\n",
      "[ training ] training loss = 0.009739411063492298\n",
      "[ eval ] validation loss = 0.015171485021710396\n",
      "[ epoch ] 263\n",
      "[ training ] training loss = 0.009711072780191898\n",
      "[ eval ] validation loss = 0.015131686814129353\n",
      "[ epoch ] 264\n",
      "[ training ] training loss = 0.00968344695866108\n",
      "[ eval ] validation loss = 0.015092737972736359\n",
      "[ epoch ] 265\n",
      "[ training ] training loss = 0.009656518697738647\n",
      "[ eval ] validation loss = 0.01505462545901537\n",
      "[ epoch ] 266\n",
      "[ training ] training loss = 0.009630260989069939\n",
      "[ eval ] validation loss = 0.015017333440482616\n",
      "[ epoch ] 267\n",
      "[ training ] training loss = 0.00960466731339693\n",
      "[ eval ] validation loss = 0.014980834908783436\n",
      "[ epoch ] 268\n",
      "[ training ] training loss = 0.009579723700881004\n",
      "[ eval ] validation loss = 0.01494511216878891\n",
      "[ epoch ] 269\n",
      "[ training ] training loss = 0.009555397555232048\n",
      "[ eval ] validation loss = 0.01491015125066042\n",
      "[ epoch ] 270\n",
      "[ training ] training loss = 0.009531688876450062\n",
      "[ eval ] validation loss = 0.014875935390591621\n",
      "[ epoch ] 271\n",
      "[ training ] training loss = 0.00950857624411583\n",
      "[ eval ] validation loss = 0.0148424431681633\n",
      "[ epoch ] 272\n",
      "[ training ] training loss = 0.009486043825745583\n",
      "[ eval ] validation loss = 0.014809662476181984\n",
      "[ epoch ] 273\n",
      "[ training ] training loss = 0.009464079514145851\n",
      "[ eval ] validation loss = 0.014777570962905884\n",
      "[ epoch ] 274\n",
      "[ training ] training loss = 0.009442666545510292\n",
      "[ eval ] validation loss = 0.014746157452464104\n",
      "[ epoch ] 275\n",
      "[ training ] training loss = 0.009421797469258308\n",
      "[ eval ] validation loss = 0.014715406112372875\n",
      "[ epoch ] 276\n",
      "[ training ] training loss = 0.009401444345712662\n",
      "[ eval ] validation loss = 0.014685305766761303\n",
      "[ epoch ] 277\n",
      "[ training ] training loss = 0.0093816127628088\n",
      "[ eval ] validation loss = 0.014655837789177895\n",
      "[ epoch ] 278\n",
      "[ training ] training loss = 0.00936227198690176\n",
      "[ eval ] validation loss = 0.014626989141106606\n",
      "[ epoch ] 279\n",
      "[ training ] training loss = 0.009343419224023819\n",
      "[ eval ] validation loss = 0.01459873653948307\n",
      "[ epoch ] 280\n",
      "[ training ] training loss = 0.009325043298304081\n",
      "[ eval ] validation loss = 0.01457107625901699\n",
      "[ epoch ] 281\n",
      "[ training ] training loss = 0.009307129308581352\n",
      "[ eval ] validation loss = 0.014543998055160046\n",
      "[ epoch ] 282\n",
      "[ training ] training loss = 0.009289667010307312\n",
      "[ eval ] validation loss = 0.014517483301460743\n",
      "[ epoch ] 283\n",
      "[ training ] training loss = 0.009272643364965916\n",
      "[ eval ] validation loss = 0.01449151523411274\n",
      "[ epoch ] 284\n",
      "[ training ] training loss = 0.009256047196686268\n",
      "[ eval ] validation loss = 0.014466091990470886\n",
      "[ epoch ] 285\n",
      "[ training ] training loss = 0.009239865466952324\n",
      "[ eval ] validation loss = 0.014441194012761116\n",
      "[ epoch ] 286\n",
      "[ training ] training loss = 0.009224092587828636\n",
      "[ eval ] validation loss = 0.014416811056435108\n",
      "[ epoch ] 287\n",
      "[ training ] training loss = 0.009208711795508862\n",
      "[ eval ] validation loss = 0.014392929151654243\n",
      "[ epoch ] 288\n",
      "[ training ] training loss = 0.009193725883960724\n",
      "[ eval ] validation loss = 0.014369538053870201\n",
      "[ epoch ] 289\n",
      "[ training ] training loss = 0.009179117158055305\n",
      "[ eval ] validation loss = 0.01434662751853466\n",
      "[ epoch ] 290\n",
      "[ training ] training loss = 0.009164875373244286\n",
      "[ eval ] validation loss = 0.014324193820357323\n",
      "[ epoch ] 291\n",
      "[ training ] training loss = 0.009150990284979343\n",
      "[ eval ] validation loss = 0.01430221926420927\n",
      "[ epoch ] 292\n",
      "[ training ] training loss = 0.009137451648712158\n",
      "[ eval ] validation loss = 0.01428068894892931\n",
      "[ epoch ] 293\n",
      "[ training ] training loss = 0.009124253876507282\n",
      "[ eval ] validation loss = 0.01425960473716259\n",
      "[ epoch ] 294\n",
      "[ training ] training loss = 0.00911138765513897\n",
      "[ eval ] validation loss = 0.014238941483199596\n",
      "[ epoch ] 295\n",
      "[ training ] training loss = 0.009098849259316921\n",
      "[ eval ] validation loss = 0.014218701049685478\n",
      "[ epoch ] 296\n",
      "[ training ] training loss = 0.009086621925234795\n",
      "[ eval ] validation loss = 0.014198876917362213\n",
      "[ epoch ] 297\n",
      "[ training ] training loss = 0.009074704721570015\n",
      "[ eval ] validation loss = 0.014179451391100883\n",
      "[ epoch ] 298\n",
      "[ training ] training loss = 0.009063085541129112\n",
      "[ eval ] validation loss = 0.014160419814288616\n",
      "[ epoch ] 299\n",
      "[ training ] training loss = 0.009051761589944363\n",
      "[ eval ] validation loss = 0.014141772873699665\n",
      "[ epoch ] 300\n",
      "[ training ] training loss = 0.00904072169214487\n",
      "[ eval ] validation loss = 0.014123504981398582\n",
      "[ epoch ] 301\n",
      "[ training ] training loss = 0.00902995653450489\n",
      "[ eval ] validation loss = 0.014105592854321003\n",
      "[ epoch ] 302\n",
      "[ training ] training loss = 0.009019467048346996\n",
      "[ eval ] validation loss = 0.014088052324950695\n",
      "[ epoch ] 303\n",
      "[ training ] training loss = 0.009009236469864845\n",
      "[ eval ] validation loss = 0.014070853590965271\n",
      "[ epoch ] 304\n",
      "[ training ] training loss = 0.00899926945567131\n",
      "[ eval ] validation loss = 0.01405399851500988\n",
      "[ epoch ] 305\n",
      "[ training ] training loss = 0.008989549241960049\n",
      "[ eval ] validation loss = 0.014037495478987694\n",
      "[ epoch ] 306\n",
      "[ training ] training loss = 0.008980072103440762\n",
      "[ eval ] validation loss = 0.014021312817931175\n",
      "[ epoch ] 307\n",
      "[ training ] training loss = 0.008970835246145725\n",
      "[ eval ] validation loss = 0.0140054477378726\n",
      "[ epoch ] 308\n",
      "[ training ] training loss = 0.008961828425526619\n",
      "[ eval ] validation loss = 0.013989908620715141\n",
      "[ epoch ] 309\n",
      "[ training ] training loss = 0.008953051641583443\n",
      "[ eval ] validation loss = 0.013974672183394432\n",
      "[ epoch ] 310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ training ] training loss = 0.008944490924477577\n",
      "[ eval ] validation loss = 0.013959737494587898\n",
      "[ epoch ] 311\n",
      "[ training ] training loss = 0.008936150930821896\n",
      "[ eval ] validation loss = 0.01394509058445692\n",
      "[ epoch ] 312\n",
      "[ training ] training loss = 0.008928020484745502\n",
      "[ eval ] validation loss = 0.013930732384324074\n",
      "[ epoch ] 313\n",
      "[ training ] training loss = 0.008920090273022652\n",
      "[ eval ] validation loss = 0.013916654512286186\n",
      "[ epoch ] 314\n",
      "[ training ] training loss = 0.00891236774623394\n",
      "[ eval ] validation loss = 0.01390286348760128\n",
      "[ epoch ] 315\n",
      "[ training ] training loss = 0.008904833346605301\n",
      "[ eval ] validation loss = 0.013889342546463013\n",
      "[ epoch ] 316\n",
      "[ training ] training loss = 0.00889748614281416\n",
      "[ eval ] validation loss = 0.013876083306968212\n",
      "[ epoch ] 317\n",
      "[ training ] training loss = 0.008890326134860516\n",
      "[ eval ] validation loss = 0.013863081112504005\n",
      "[ epoch ] 318\n",
      "[ training ] training loss = 0.008883347734808922\n",
      "[ eval ] validation loss = 0.013850326649844646\n",
      "[ epoch ] 319\n",
      "[ training ] training loss = 0.008876542560756207\n",
      "[ eval ] validation loss = 0.013837823644280434\n",
      "[ epoch ] 320\n",
      "[ training ] training loss = 0.00886991061270237\n",
      "[ eval ] validation loss = 0.013825567439198494\n",
      "[ epoch ] 321\n",
      "[ training ] training loss = 0.00886344350874424\n",
      "[ eval ] validation loss = 0.01381353847682476\n",
      "[ epoch ] 322\n",
      "[ training ] training loss = 0.00885714590549469\n",
      "[ eval ] validation loss = 0.013801753520965576\n",
      "[ epoch ] 323\n",
      "[ training ] training loss = 0.008850997313857079\n",
      "[ eval ] validation loss = 0.013790188357234001\n",
      "[ epoch ] 324\n",
      "[ training ] training loss = 0.008845007047057152\n",
      "[ eval ] validation loss = 0.013778842985630035\n",
      "[ epoch ] 325\n",
      "[ training ] training loss = 0.008839169517159462\n",
      "[ eval ] validation loss = 0.013767717406153679\n",
      "[ epoch ] 326\n",
      "[ training ] training loss = 0.008833472616970539\n",
      "[ eval ] validation loss = 0.013756809756159782\n",
      "[ epoch ] 327\n",
      "[ training ] training loss = 0.008827927522361279\n",
      "[ eval ] validation loss = 0.013746106997132301\n",
      "[ epoch ] 328\n",
      "[ training ] training loss = 0.008822517469525337\n",
      "[ eval ] validation loss = 0.01373561006039381\n",
      "[ epoch ] 329\n",
      "[ training ] training loss = 0.008817237801849842\n",
      "[ eval ] validation loss = 0.013725310564041138\n",
      "[ epoch ] 330\n",
      "[ training ] training loss = 0.008812101557850838\n",
      "[ eval ] validation loss = 0.013715207576751709\n",
      "[ epoch ] 331\n",
      "[ training ] training loss = 0.008807088248431683\n",
      "[ eval ] validation loss = 0.013705296441912651\n",
      "[ epoch ] 332\n",
      "[ training ] training loss = 0.008802199736237526\n",
      "[ eval ] validation loss = 0.013695565983653069\n",
      "[ epoch ] 333\n",
      "[ training ] training loss = 0.008797438815236092\n",
      "[ eval ] validation loss = 0.013686025515198708\n",
      "[ epoch ] 334\n",
      "[ training ] training loss = 0.008792797103524208\n",
      "[ eval ] validation loss = 0.013676662929356098\n",
      "[ epoch ] 335\n",
      "[ training ] training loss = 0.008788271807134151\n",
      "[ eval ] validation loss = 0.013667479157447815\n",
      "[ epoch ] 336\n",
      "[ training ] training loss = 0.008783860132098198\n",
      "[ eval ] validation loss = 0.013658462092280388\n",
      "[ epoch ] 337\n",
      "[ training ] training loss = 0.00877955462783575\n",
      "[ eval ] validation loss = 0.01364961452782154\n",
      "[ epoch ] 338\n",
      "[ training ] training loss = 0.008775362744927406\n",
      "[ eval ] validation loss = 0.013640938326716423\n",
      "[ epoch ] 339\n",
      "[ training ] training loss = 0.008771274238824844\n",
      "[ eval ] validation loss = 0.01363241858780384\n",
      "[ epoch ] 340\n",
      "[ training ] training loss = 0.008767293766140938\n",
      "[ eval ] validation loss = 0.013624059036374092\n",
      "[ epoch ] 341\n",
      "[ training ] training loss = 0.008763409219682217\n",
      "[ eval ] validation loss = 0.01361585222184658\n",
      "[ epoch ] 342\n",
      "[ training ] training loss = 0.00875962246209383\n",
      "[ eval ] validation loss = 0.013607801869511604\n",
      "[ epoch ] 343\n",
      "[ training ] training loss = 0.008755930699408054\n",
      "[ eval ] validation loss = 0.013599900528788567\n",
      "[ epoch ] 344\n",
      "[ training ] training loss = 0.008752333000302315\n",
      "[ eval ] validation loss = 0.01359214074909687\n",
      "[ epoch ] 345\n",
      "[ training ] training loss = 0.008748825639486313\n",
      "[ eval ] validation loss = 0.013584527187049389\n",
      "[ epoch ] 346\n",
      "[ training ] training loss = 0.008745407685637474\n",
      "[ eval ] validation loss = 0.01357705146074295\n",
      "[ epoch ] 347\n",
      "[ training ] training loss = 0.008742071688175201\n",
      "[ eval ] validation loss = 0.013569707982242107\n",
      "[ epoch ] 348\n",
      "[ training ] training loss = 0.008738820441067219\n",
      "[ eval ] validation loss = 0.013562506064772606\n",
      "[ epoch ] 349\n",
      "[ training ] training loss = 0.008735652081668377\n",
      "[ eval ] validation loss = 0.01355543453246355\n",
      "[ epoch ] 350\n",
      "[ training ] training loss = 0.008732563816010952\n",
      "[ eval ] validation loss = 0.013548491522669792\n",
      "[ epoch ] 351\n",
      "[ training ] training loss = 0.008729551918804646\n",
      "[ eval ] validation loss = 0.01354166679084301\n",
      "[ epoch ] 352\n",
      "[ training ] training loss = 0.008726617321372032\n",
      "[ eval ] validation loss = 0.0135349677875638\n",
      "[ epoch ] 353\n",
      "[ training ] training loss = 0.008723755367100239\n",
      "[ eval ] validation loss = 0.01352839358150959\n",
      "[ epoch ] 354\n",
      "[ training ] training loss = 0.008720966055989265\n",
      "[ eval ] validation loss = 0.013521937653422356\n",
      "[ epoch ] 355\n",
      "[ training ] training loss = 0.008718249388039112\n",
      "[ eval ] validation loss = 0.013515597209334373\n",
      "[ epoch ] 356\n",
      "[ training ] training loss = 0.008715596981346607\n",
      "[ eval ] validation loss = 0.013509370386600494\n",
      "[ epoch ] 357\n",
      "[ training ] training loss = 0.008713014423847198\n",
      "[ eval ] validation loss = 0.013503259047865868\n",
      "[ epoch ] 358\n",
      "[ training ] training loss = 0.008710496127605438\n",
      "[ eval ] validation loss = 0.013497255742549896\n",
      "[ epoch ] 359\n",
      "[ training ] training loss = 0.008708037436008453\n",
      "[ eval ] validation loss = 0.013491354882717133\n",
      "[ epoch ] 360\n",
      "[ training ] training loss = 0.008705643936991692\n",
      "[ eval ] validation loss = 0.013485556468367577\n",
      "[ epoch ] 361\n",
      "[ training ] training loss = 0.00870331097394228\n",
      "[ eval ] validation loss = 0.013479871675372124\n",
      "[ epoch ] 362\n",
      "[ training ] training loss = 0.008701035752892494\n",
      "[ eval ] validation loss = 0.013474280014634132\n",
      "[ epoch ] 363\n",
      "[ training ] training loss = 0.008698816411197186\n",
      "[ eval ] validation loss = 0.013468794524669647\n",
      "[ epoch ] 364\n",
      "[ training ] training loss = 0.008696655742824078\n",
      "[ eval ] validation loss = 0.013463403098285198\n",
      "[ epoch ] 365\n",
      "[ training ] training loss = 0.008694547228515148\n",
      "[ eval ] validation loss = 0.013458097353577614\n",
      "[ epoch ] 366\n",
      "[ training ] training loss = 0.008692493662238121\n",
      "[ eval ] validation loss = 0.013452892191708088\n",
      "[ epoch ] 367\n",
      "[ training ] training loss = 0.008690491318702698\n",
      "[ eval ] validation loss = 0.013447782024741173\n",
      "[ epoch ] 368\n",
      "[ training ] training loss = 0.008688538335263729\n",
      "[ eval ] validation loss = 0.013442756608128548\n",
      "[ epoch ] 369\n",
      "[ training ] training loss = 0.008686636574566364\n",
      "[ eval ] validation loss = 0.013437817804515362\n",
      "[ epoch ] 370\n",
      "[ training ] training loss = 0.008684777654707432\n",
      "[ eval ] validation loss = 0.013432961888611317\n",
      "[ epoch ] 371\n",
      "[ training ] training loss = 0.008682969026267529\n",
      "[ eval ] validation loss = 0.013428199104964733\n",
      "[ epoch ] 372\n",
      "[ training ] training loss = 0.008681206963956356\n",
      "[ eval ] validation loss = 0.013423511758446693\n",
      "[ epoch ] 373\n",
      "[ training ] training loss = 0.008679484948515892\n",
      "[ eval ] validation loss = 0.013418908230960369\n",
      "[ epoch ] 374\n",
      "[ training ] training loss = 0.008677815087139606\n",
      "[ eval ] validation loss = 0.013414384797215462\n",
      "[ epoch ] 375\n",
      "[ training ] training loss = 0.008676180616021156\n",
      "[ eval ] validation loss = 0.013409934937953949\n",
      "[ epoch ] 376\n",
      "[ training ] training loss = 0.008674588054418564\n",
      "[ eval ] validation loss = 0.013405573554337025\n",
      "[ epoch ] 377\n",
      "[ training ] training loss = 0.008673036471009254\n",
      "[ eval ] validation loss = 0.013401279225945473\n",
      "[ epoch ] 378\n",
      "[ training ] training loss = 0.008671516552567482\n",
      "[ eval ] validation loss = 0.013397063128650188\n",
      "[ epoch ] 379\n",
      "[ training ] training loss = 0.008670045994222164\n",
      "[ eval ] validation loss = 0.013392915017902851\n",
      "[ epoch ] 380\n",
      "[ training ] training loss = 0.00866860430687666\n",
      "[ eval ] validation loss = 0.013388840481638908\n",
      "[ epoch ] 381\n",
      "[ training ] training loss = 0.008667203597724438\n",
      "[ eval ] validation loss = 0.013384826481342316\n",
      "[ epoch ] 382\n",
      "[ training ] training loss = 0.008665837347507477\n",
      "[ eval ] validation loss = 0.013380889780819416\n",
      "[ epoch ] 383\n",
      "[ training ] training loss = 0.008664505556225777\n",
      "[ eval ] validation loss = 0.013377013616263866\n",
      "[ epoch ] 384\n",
      "[ training ] training loss = 0.008663207292556763\n",
      "[ eval ] validation loss = 0.013373198918998241\n",
      "[ epoch ] 385\n",
      "[ training ] training loss = 0.00866194162517786\n",
      "[ eval ] validation loss = 0.013369455933570862\n",
      "[ epoch ] 386\n",
      "[ training ] training loss = 0.008660703897476196\n",
      "[ eval ] validation loss = 0.013365773484110832\n",
      "[ epoch ] 387\n",
      "[ training ] training loss = 0.008659498766064644\n",
      "[ eval ] validation loss = 0.01336215902119875\n",
      "[ epoch ] 388\n",
      "[ training ] training loss = 0.008658329024910927\n",
      "[ eval ] validation loss = 0.013358602300286293\n",
      "[ epoch ] 389\n",
      "[ training ] training loss = 0.008657185360789299\n",
      "[ eval ] validation loss = 0.013355103321373463\n",
      "[ epoch ] 390\n",
      "[ training ] training loss = 0.008656073361635208\n",
      "[ eval ] validation loss = 0.013351665809750557\n",
      "[ epoch ] 391\n",
      "[ training ] training loss = 0.008654983714222908\n",
      "[ eval ] validation loss = 0.013348273932933807\n",
      "[ epoch ] 392\n",
      "[ training ] training loss = 0.00865392480045557\n",
      "[ eval ] validation loss = 0.013344946317374706\n",
      "[ epoch ] 393\n",
      "[ training ] training loss = 0.008652890101075172\n",
      "[ eval ] validation loss = 0.01334168016910553\n",
      "[ epoch ] 394\n",
      "[ training ] training loss = 0.008651885204017162\n",
      "[ eval ] validation loss = 0.013338463380932808\n",
      "[ epoch ] 395\n",
      "[ training ] training loss = 0.008650902658700943\n",
      "[ eval ] validation loss = 0.013335293158888817\n",
      "[ epoch ] 396\n",
      "[ training ] training loss = 0.008649946190416813\n",
      "[ eval ] validation loss = 0.013332180678844452\n",
      "[ epoch ] 397\n",
      "[ training ] training loss = 0.008649014867842197\n",
      "[ eval ] validation loss = 0.013329113833606243\n",
      "[ epoch ] 398\n",
      "[ training ] training loss = 0.0086481012403965\n",
      "[ eval ] validation loss = 0.013326103799045086\n",
      "[ epoch ] 399\n",
      "[ training ] training loss = 0.008647219277918339\n",
      "[ eval ] validation loss = 0.013323145918548107\n",
      "[ epoch ] 400\n",
      "[ training ] training loss = 0.008646350353956223\n",
      "[ eval ] validation loss = 0.01332023460417986\n",
      "[ epoch ] 401\n",
      "[ training ] training loss = 0.008645511232316494\n",
      "[ eval ] validation loss = 0.01331736333668232\n",
      "[ epoch ] 402\n",
      "[ training ] training loss = 0.008644689805805683\n",
      "[ eval ] validation loss = 0.013314545154571533\n",
      "[ epoch ] 403\n",
      "[ training ] training loss = 0.008643889799714088\n",
      "[ eval ] validation loss = 0.013311771675944328\n",
      "[ epoch ] 404\n",
      "[ training ] training loss = 0.008643107488751411\n",
      "[ eval ] validation loss = 0.01330904196947813\n",
      "[ epoch ] 405\n",
      "[ training ] training loss = 0.008642351254820824\n",
      "[ eval ] validation loss = 0.013306356966495514\n",
      "[ epoch ] 406\n",
      "[ training ] training loss = 0.008641607128083706\n",
      "[ eval ] validation loss = 0.0133037269115448\n",
      "[ epoch ] 407\n",
      "[ training ] training loss = 0.008640884421765804\n",
      "[ eval ] validation loss = 0.013301128521561623\n",
      "[ epoch ] 408\n",
      "[ training ] training loss = 0.008640178479254246\n",
      "[ eval ] validation loss = 0.013298571109771729\n",
      "[ epoch ] 409\n",
      "[ training ] training loss = 0.008639492094516754\n",
      "[ eval ] validation loss = 0.013296057470142841\n",
      "[ epoch ] 410\n",
      "[ training ] training loss = 0.008638824336230755\n",
      "[ eval ] validation loss = 0.013293586671352386\n",
      "[ epoch ] 411\n",
      "[ training ] training loss = 0.00863817147910595\n",
      "[ eval ] validation loss = 0.01329115591943264\n",
      "[ epoch ] 412\n",
      "[ training ] training loss = 0.008637534454464912\n",
      "[ eval ] validation loss = 0.013288751244544983\n",
      "[ epoch ] 413\n",
      "[ training ] training loss = 0.00863691233098507\n",
      "[ eval ] validation loss = 0.013286398723721504\n",
      "[ epoch ] 414\n",
      "[ training ] training loss = 0.008636308833956718\n",
      "[ eval ] validation loss = 0.013284072279930115\n",
      "[ epoch ] 415\n",
      "[ training ] training loss = 0.008635721169412136\n",
      "[ eval ] validation loss = 0.013281784951686859\n",
      "[ epoch ] 416\n",
      "[ training ] training loss = 0.008635146543383598\n",
      "[ eval ] validation loss = 0.013279544189572334\n",
      "[ epoch ] 417\n",
      "[ training ] training loss = 0.00863458402454853\n",
      "[ eval ] validation loss = 0.013277326710522175\n",
      "[ epoch ] 418\n",
      "[ training ] training loss = 0.008634038269519806\n",
      "[ eval ] validation loss = 0.013275153934955597\n",
      "[ epoch ] 419\n",
      "[ training ] training loss = 0.008633502759039402\n",
      "[ eval ] validation loss = 0.013273012824356556\n",
      "[ epoch ] 420\n",
      "[ training ] training loss = 0.00863298587501049\n",
      "[ eval ] validation loss = 0.013270904310047626\n",
      "[ epoch ] 421\n",
      "[ training ] training loss = 0.0086324792355299\n",
      "[ eval ] validation loss = 0.013268825598061085\n",
      "[ epoch ] 422\n",
      "[ training ] training loss = 0.008631987497210503\n",
      "[ eval ] validation loss = 0.013266781345009804\n",
      "[ epoch ] 423\n",
      "[ training ] training loss = 0.008631506934762001\n",
      "[ eval ] validation loss = 0.013264767825603485\n",
      "[ epoch ] 424\n",
      "[ training ] training loss = 0.008631037548184395\n",
      "[ eval ] validation loss = 0.01326278317719698\n",
      "[ epoch ] 425\n",
      "[ training ] training loss = 0.008630579337477684\n",
      "[ eval ] validation loss = 0.013260838575661182\n",
      "[ epoch ] 426\n",
      "[ training ] training loss = 0.008630136027932167\n",
      "[ eval ] validation loss = 0.013258921913802624\n",
      "[ epoch ] 427\n",
      "[ training ] training loss = 0.008629703894257545\n",
      "[ eval ] validation loss = 0.013257029466331005\n",
      "[ epoch ] 428\n",
      "[ training ] training loss = 0.008629277348518372\n",
      "[ eval ] validation loss = 0.013255171477794647\n",
      "[ epoch ] 429\n",
      "[ training ] training loss = 0.008628865703940392\n",
      "[ eval ] validation loss = 0.013253343291580677\n",
      "[ epoch ] 430\n",
      "[ training ] training loss = 0.008628461509943008\n",
      "[ eval ] validation loss = 0.013251538388431072\n",
      "[ epoch ] 431\n",
      "[ training ] training loss = 0.008628071285784245\n",
      "[ eval ] validation loss = 0.013249760493636131\n",
      "[ epoch ] 432\n",
      "[ training ] training loss = 0.008627688512206078\n",
      "[ eval ] validation loss = 0.013248016126453876\n",
      "[ epoch ] 433\n",
      "[ training ] training loss = 0.008627315983176231\n",
      "[ eval ] validation loss = 0.01324629969894886\n",
      "[ epoch ] 434\n",
      "[ training ] training loss = 0.008626952767372131\n",
      "[ eval ] validation loss = 0.01324460469186306\n",
      "[ epoch ] 435\n",
      "[ training ] training loss = 0.008626596070826054\n",
      "[ eval ] validation loss = 0.013242935761809349\n",
      "[ epoch ] 436\n",
      "[ training ] training loss = 0.008626251481473446\n",
      "[ eval ] validation loss = 0.013241291046142578\n",
      "[ epoch ] 437\n",
      "[ training ] training loss = 0.008625917136669159\n",
      "[ eval ] validation loss = 0.013239676132798195\n",
      "[ epoch ] 438\n",
      "[ training ] training loss = 0.008625583723187447\n",
      "[ eval ] validation loss = 0.013238082639873028\n",
      "[ epoch ] 439\n",
      "[ training ] training loss = 0.008625267073512077\n",
      "[ eval ] validation loss = 0.013236512430012226\n",
      "[ epoch ] 440\n",
      "[ training ] training loss = 0.008624956011772156\n",
      "[ eval ] validation loss = 0.013234967365860939\n",
      "[ epoch ] 441\n",
      "[ training ] training loss = 0.008624653331935406\n",
      "[ eval ] validation loss = 0.01323343813419342\n",
      "[ epoch ] 442\n",
      "[ training ] training loss = 0.008624354377388954\n",
      "[ eval ] validation loss = 0.013231937773525715\n",
      "[ epoch ] 443\n",
      "[ training ] training loss = 0.0086240628734231\n",
      "[ eval ] validation loss = 0.013230465352535248\n",
      "[ epoch ] 444\n",
      "[ training ] training loss = 0.008623786270618439\n",
      "[ eval ] validation loss = 0.013229010626673698\n",
      "[ epoch ] 445\n",
      "[ training ] training loss = 0.008623511530458927\n",
      "[ eval ] validation loss = 0.013227574527263641\n",
      "[ epoch ] 446\n",
      "[ training ] training loss = 0.008623243309557438\n",
      "[ eval ] validation loss = 0.01322617381811142\n",
      "[ epoch ] 447\n",
      "[ training ] training loss = 0.00862298347055912\n",
      "[ eval ] validation loss = 0.013224789872765541\n",
      "[ epoch ] 448\n",
      "[ training ] training loss = 0.008622727356851101\n",
      "[ eval ] validation loss = 0.013223419897258282\n",
      "[ epoch ] 449\n",
      "[ training ] training loss = 0.008622480556368828\n",
      "[ eval ] validation loss = 0.013222074136137962\n",
      "[ epoch ] 450\n",
      "[ training ] training loss = 0.00862223468720913\n",
      "[ eval ] validation loss = 0.013220747001469135\n",
      "[ epoch ] 451\n",
      "[ training ] training loss = 0.008622003719210625\n",
      "[ eval ] validation loss = 0.013219441287219524\n",
      "[ epoch ] 452\n",
      "[ training ] training loss = 0.00862177275121212\n",
      "[ eval ] validation loss = 0.01321814488619566\n",
      "[ epoch ] 453\n",
      "[ training ] training loss = 0.008621550165116787\n",
      "[ eval ] validation loss = 0.013216872699558735\n",
      "[ epoch ] 454\n",
      "[ training ] training loss = 0.008621332235634327\n",
      "[ eval ] validation loss = 0.013215619139373302\n",
      "[ epoch ] 455\n",
      "[ training ] training loss = 0.008621116168797016\n",
      "[ eval ] validation loss = 0.013214381411671638\n",
      "[ epoch ] 456\n",
      "[ training ] training loss = 0.008620912209153175\n",
      "[ eval ] validation loss = 0.013213175348937511\n",
      "[ epoch ] 457\n",
      "[ training ] training loss = 0.00862070731818676\n",
      "[ eval ] validation loss = 0.013211974874138832\n",
      "[ epoch ] 458\n",
      "[ training ] training loss = 0.008620509877800941\n",
      "[ eval ] validation loss = 0.013210800476372242\n",
      "[ epoch ] 459\n",
      "[ training ] training loss = 0.00862031988799572\n",
      "[ eval ] validation loss = 0.01320964377373457\n",
      "[ epoch ] 460\n",
      "[ training ] training loss = 0.008620129898190498\n",
      "[ eval ] validation loss = 0.013208496384322643\n",
      "[ epoch ] 461\n",
      "[ training ] training loss = 0.008619948290288448\n",
      "[ eval ] validation loss = 0.013207370415329933\n",
      "[ epoch ] 462\n",
      "[ training ] training loss = 0.008619772270321846\n",
      "[ eval ] validation loss = 0.013206263072788715\n",
      "[ epoch ] 463\n",
      "[ training ] training loss = 0.008619597181677818\n",
      "[ eval ] validation loss = 0.013205165043473244\n",
      "[ epoch ] 464\n",
      "[ training ] training loss = 0.008619428612291813\n",
      "[ eval ] validation loss = 0.013204087503254414\n",
      "[ epoch ] 465\n",
      "[ training ] training loss = 0.00861926469951868\n",
      "[ eval ] validation loss = 0.01320301741361618\n",
      "[ epoch ] 466\n",
      "[ training ] training loss = 0.008619101718068123\n",
      "[ eval ] validation loss = 0.013201969675719738\n",
      "[ epoch ] 467\n",
      "[ training ] training loss = 0.008618947118520737\n",
      "[ eval ] validation loss = 0.013200946152210236\n",
      "[ epoch ] 468\n",
      "[ training ] training loss = 0.008618793450295925\n",
      "[ eval ] validation loss = 0.01319993007928133\n",
      "[ epoch ] 469\n",
      "[ training ] training loss = 0.008618643507361412\n",
      "[ eval ] validation loss = 0.013198928907513618\n",
      "[ epoch ] 470\n",
      "[ training ] training loss = 0.008618498221039772\n",
      "[ eval ] validation loss = 0.013197940774261951\n",
      "[ epoch ] 471\n",
      "[ training ] training loss = 0.008618353866040707\n",
      "[ eval ] validation loss = 0.013196969404816628\n",
      "[ epoch ] 472\n",
      "[ training ] training loss = 0.008618218824267387\n",
      "[ eval ] validation loss = 0.013196011073887348\n",
      "[ epoch ] 473\n",
      "[ training ] training loss = 0.008618082851171494\n",
      "[ eval ] validation loss = 0.013195065781474113\n",
      "[ epoch ] 474\n",
      "[ training ] training loss = 0.008617953397333622\n",
      "[ eval ] validation loss = 0.013194131664931774\n",
      "[ epoch ] 475\n",
      "[ training ] training loss = 0.0086178258061409\n",
      "[ eval ] validation loss = 0.013193219900131226\n",
      "[ epoch ] 476\n",
      "[ training ] training loss = 0.008617700077593327\n",
      "[ eval ] validation loss = 0.013192313723266125\n",
      "[ epoch ] 477\n",
      "[ training ] training loss = 0.008617579005658627\n",
      "[ eval ] validation loss = 0.013191411271691322\n",
      "[ epoch ] 478\n",
      "[ training ] training loss = 0.008617458865046501\n",
      "[ eval ] validation loss = 0.013190535828471184\n",
      "[ epoch ] 479\n",
      "[ training ] training loss = 0.008617347106337547\n",
      "[ eval ] validation loss = 0.013189663179218769\n",
      "[ epoch ] 480\n",
      "[ training ] training loss = 0.00861723069101572\n",
      "[ eval ] validation loss = 0.013188809156417847\n",
      "[ epoch ] 481\n",
      "[ training ] training loss = 0.00861712358891964\n",
      "[ eval ] validation loss = 0.01318796444684267\n",
      "[ epoch ] 482\n",
      "[ training ] training loss = 0.008617015555500984\n",
      "[ eval ] validation loss = 0.013187138363718987\n",
      "[ epoch ] 483\n",
      "[ training ] training loss = 0.008616913110017776\n",
      "[ eval ] validation loss = 0.013186318799853325\n",
      "[ epoch ] 484\n",
      "[ training ] training loss = 0.008616813458502293\n",
      "[ eval ] validation loss = 0.013185515999794006\n",
      "[ epoch ] 485\n",
      "[ training ] training loss = 0.008616712875664234\n",
      "[ eval ] validation loss = 0.013184720650315285\n",
      "[ epoch ] 486\n",
      "[ training ] training loss = 0.008616614155471325\n",
      "[ eval ] validation loss = 0.013183931820094585\n",
      "[ epoch ] 487\n",
      "[ training ] training loss = 0.00861651822924614\n",
      "[ eval ] validation loss = 0.013183156959712505\n",
      "[ epoch ] 488\n",
      "[ training ] training loss = 0.008616428822278976\n",
      "[ eval ] validation loss = 0.013182397000491619\n",
      "[ epoch ] 489\n",
      "[ training ] training loss = 0.008616340346634388\n",
      "[ eval ] validation loss = 0.013181651011109352\n",
      "[ epoch ] 490\n",
      "[ training ] training loss = 0.00861625000834465\n",
      "[ eval ] validation loss = 0.013180908747017384\n",
      "[ epoch ] 491\n",
      "[ training ] training loss = 0.00861616525799036\n",
      "[ eval ] validation loss = 0.013180183246731758\n",
      "[ epoch ] 492\n",
      "[ training ] training loss = 0.008616084232926369\n",
      "[ eval ] validation loss = 0.01317946333438158\n",
      "[ epoch ] 493\n",
      "[ training ] training loss = 0.008616003207862377\n",
      "[ eval ] validation loss = 0.013178752735257149\n",
      "[ epoch ] 494\n",
      "[ training ] training loss = 0.00861592497676611\n",
      "[ eval ] validation loss = 0.013178055174648762\n",
      "[ epoch ] 495\n",
      "[ training ] training loss = 0.008615845814347267\n",
      "[ eval ] validation loss = 0.013177354820072651\n",
      "[ epoch ] 496\n",
      "[ training ] training loss = 0.008615771308541298\n",
      "[ eval ] validation loss = 0.013176674023270607\n",
      "[ epoch ] 497\n",
      "[ training ] training loss = 0.008615700528025627\n",
      "[ eval ] validation loss = 0.013176006264984608\n",
      "[ epoch ] 498\n",
      "[ training ] training loss = 0.008615627884864807\n",
      "[ eval ] validation loss = 0.013175344094634056\n",
      "[ epoch ] 499\n",
      "[ training ] training loss = 0.008615557104349136\n",
      "[ eval ] validation loss = 0.0131746931001544\n",
      "linear.weight = tensor([2.9827], requires_grad=True), linear.bias = tensor([-0.9722], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# set training routine\n",
    "lr = 1e-1\n",
    "n_epochs = 500\n",
    "\n",
    "# train model\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"[ epoch ]\", epoch)\n",
    "    yhat = bias + weight * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "    losses.append(loss.item())\n",
    "    print(\"[ training ] training loss = {}\".format(loss))\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update weight and bias\n",
    "    with torch.no_grad():\n",
    "        bias -= lr * bias.grad\n",
    "        weight -= lr * weight.grad\n",
    "    # zero out grads\n",
    "    bias.grad.zero_()\n",
    "    weight.grad.zero_()\n",
    "\n",
    "    # eval\n",
    "    with torch.no_grad():\n",
    "        yhat = bias + weight * x_val_tensor\n",
    "        error = y_val_tensor - yhat\n",
    "        val_loss = (error ** 2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "        print(\"[ eval ] validation loss = {}\".format(val_loss))\n",
    "    \n",
    "print(\"linear.weight = {}, linear.bias = {}\".format(weight, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(n_epochs), losses, label='train_loss', color='b')\n",
    "plt.scatter(range(n_epochs), val_losses, label='val_loss', color='r')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_p36': conda)",
   "language": "python",
   "name": "python361064bitpytorchp36conda143b13e29122453f97130b8bdfe91e87"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
